"""
Model Router - Intelligens modell v√°laszt√°s feladatt√≠pus alapj√°n

K√∂lts√©g + teljes√≠tm√©ny optimaliz√°l√°s:
- gpt-4o: komplex gondolkod√°s, d√∂nt√©sek
- gpt-4o-mini: √∂sszefoglal√°s, routing, egyszer≈± feladatok
- text-embedding-3-large: embedding
"""

from typing import Optional, Literal
from dataclasses import dataclass
from enum import Enum


class TaskType(Enum):
    """Feladat t√≠pusok"""
    # Komplex - gpt-4o kell
    CODE_GENERATION = "code_generation"
    CODE_REVIEW = "code_review"
    DEBUGGING = "debugging"
    ARCHITECTURE = "architecture"
    COMPLEX_REASONING = "complex_reasoning"
    AGENTIC_EXECUTION = "agentic_execution"
    
    # K√∂zepes - mindkett≈ë j√≥
    CODE_EXPLANATION = "code_explanation"
    DOCUMENTATION = "documentation"
    TRANSLATION = "translation"
    
    # Egyszer≈± - gpt-4o-mini el√©g
    SUMMARIZATION = "summarization"
    ROUTING = "routing"
    CLASSIFICATION = "classification"
    SIMPLE_QA = "simple_qa"
    FORMATTING = "formatting"
    TOOL_SELECTION = "tool_selection"


@dataclass
class ModelConfig:
    """Model konfigur√°ci√≥"""
    name: str
    max_tokens: int
    cost_per_1k_input: float  # USD
    cost_per_1k_output: float
    strengths: list


# Model defin√≠ci√≥k
MODELS = {
    "gpt-4o": ModelConfig(
        name="gpt-4o",
        max_tokens=128000,
        cost_per_1k_input=0.005,
        cost_per_1k_output=0.015,
        strengths=["reasoning", "code", "complex tasks", "agentic"]
    ),
    "gpt-4o-mini": ModelConfig(
        name="gpt-4o-mini",
        max_tokens=128000,
        cost_per_1k_input=0.00015,
        cost_per_1k_output=0.0006,
        strengths=["speed", "cost", "simple tasks", "summaries"]
    ),
    "gpt-4-turbo": ModelConfig(
        name="gpt-4-turbo",
        max_tokens=128000,
        cost_per_1k_input=0.01,
        cost_per_1k_output=0.03,
        strengths=["reasoning", "code", "legacy"]
    ),
}

# Feladat -> Model mapping
TASK_MODEL_MAP = {
    # Komplex feladatok - mindig gpt-4o
    TaskType.CODE_GENERATION: "gpt-4o",
    TaskType.CODE_REVIEW: "gpt-4o",
    TaskType.DEBUGGING: "gpt-4o",
    TaskType.ARCHITECTURE: "gpt-4o",
    TaskType.COMPLEX_REASONING: "gpt-4o",
    TaskType.AGENTIC_EXECUTION: "gpt-4o",
    
    # K√∂zepes - alapb√≥l gpt-4o, de mini is m≈±k√∂dhet
    TaskType.CODE_EXPLANATION: "gpt-4o",
    TaskType.DOCUMENTATION: "gpt-4o-mini",
    TaskType.TRANSLATION: "gpt-4o-mini",
    
    # Egyszer≈± - gpt-4o-mini
    TaskType.SUMMARIZATION: "gpt-4o-mini",
    TaskType.ROUTING: "gpt-4o-mini",
    TaskType.CLASSIFICATION: "gpt-4o-mini",
    TaskType.SIMPLE_QA: "gpt-4o-mini",
    TaskType.FORMATTING: "gpt-4o-mini",
    TaskType.TOOL_SELECTION: "gpt-4o-mini",
}


class ModelRouter:
    """
    Intelligens model router - automatikusan v√°lasztja a megfelel≈ë modellt
    """
    
    def __init__(self, default_model: str = "gpt-4o", force_model: str = None):
        """
        Args:
            default_model: Alap√©rtelmezett model ha nem tudjuk eld√∂nteni
            force_model: Ha megadva, mindig ezt haszn√°lja (override)
        """
        self.default_model = default_model
        self.force_model = force_model
        self.usage_stats = {
            "gpt-4o": {"calls": 0, "input_tokens": 0, "output_tokens": 0},
            "gpt-4o-mini": {"calls": 0, "input_tokens": 0, "output_tokens": 0},
        }
    
    def get_model_for_task(self, task_type: TaskType) -> str:
        """Modell v√°laszt√°s feladat t√≠pus alapj√°n"""
        if self.force_model:
            return self.force_model
        return TASK_MODEL_MAP.get(task_type, self.default_model)
    
    def classify_task(self, user_message: str, context: str = "") -> TaskType:
        """
        Feladat t√≠pus automatikus felismer√©se az √ºzenet alapj√°n.
        
        Ez egy egyszer≈± heurisztikus megk√∂zel√≠t√©s - k√©s≈ëbb LLM-mel is lehetne.
        """
        message_lower = user_message.lower()
        
        # K√≥d gener√°l√°s jelz≈ëk
        code_gen_keywords = [
            "√≠rj", "write", "create", "implement", "add", "hozz l√©tre",
            "k√©sz√≠ts", "make", "build", "generate", "√∫j funkci√≥", "new function",
            "add function", "√∫j oszt√°ly", "new class"
        ]
        if any(kw in message_lower for kw in code_gen_keywords):
            return TaskType.CODE_GENERATION
        
        # Debugging jelz≈ëk
        debug_keywords = [
            "hiba", "error", "bug", "fix", "jav√≠t", "debug", "nem m≈±k√∂dik",
            "doesn't work", "broken", "issue", "problem", "wrong"
        ]
        if any(kw in message_lower for kw in debug_keywords):
            return TaskType.DEBUGGING
        
        # Code review jelz≈ëk
        review_keywords = [
            "review", "ellen≈ëriz", "check", "n√©zd meg", "look at",
            "v√©lem√©nyez", "mit gondolsz", "what do you think",
            "javaslat", "suggestion", "improve", "fejleszt"
        ]
        if any(kw in message_lower for kw in review_keywords):
            return TaskType.CODE_REVIEW
        
        # √ñsszefoglal√°s jelz≈ëk
        summary_keywords = [
            "√∂sszefoglal", "summarize", "summary", "foglald √∂ssze",
            "r√∂viden", "briefly", "kivonat", "tl;dr"
        ]
        if any(kw in message_lower for kw in summary_keywords):
            return TaskType.SUMMARIZATION
        
        # Ford√≠t√°s jelz≈ëk (komment ford√≠t√°s, stb.)
        translate_keywords = [
            "ford√≠t", "translate", "hungarian", "english", "magyar",
            "angol", "komment", "comment"
        ]
        if any(kw in message_lower for kw in translate_keywords):
            return TaskType.TRANSLATION
        
        # Magyar√°zat jelz≈ëk
        explain_keywords = [
            "magyar√°z", "explain", "mi ez", "what is", "how does",
            "hogyan m≈±k√∂dik", "explain this", "mit csin√°l"
        ]
        if any(kw in message_lower for kw in explain_keywords):
            return TaskType.CODE_EXPLANATION
        
        # Egyszer≈± k√©rd√©s jelz≈ëk
        simple_qa_keywords = [
            "mi a", "what is the", "h√°ny", "how many", "melyik",
            "which", "hol van", "where is"
        ]
        if any(kw in message_lower for kw in simple_qa_keywords):
            return TaskType.SIMPLE_QA
        
        # Default: komplex reasoning (biztons√°gos v√°laszt√°s)
        return TaskType.COMPLEX_REASONING
    
    def route(self, user_message: str, context: str = "", prefer_cheap: bool = False) -> str:
        """
        Automatikus model routing.
        
        Args:
            user_message: Felhaszn√°l√≥ √ºzenete
            context: Opcion√°lis kontextus
            prefer_cheap: Ha True, olcs√≥bb modellt prefer√°l ha lehets√©ges
        
        Returns:
            Model neve
        """
        if self.force_model:
            return self.force_model
        
        task_type = self.classify_task(user_message, context)
        model = self.get_model_for_task(task_type)
        
        # Ha olcs√≥bb modellt prefer√°lunk √©s nem kritikus a feladat
        if prefer_cheap and task_type not in [
            TaskType.CODE_GENERATION,
            TaskType.DEBUGGING,
            TaskType.AGENTIC_EXECUTION
        ]:
            model = "gpt-4o-mini"
        
        print(f"[MODEL ROUTER] Task: {task_type.value} -> Model: {model}")
        return model
    
    def record_usage(self, model: str, input_tokens: int, output_tokens: int):
        """Haszn√°lat r√∂gz√≠t√©se k√∂lts√©g k√∂vet√©shez"""
        if model in self.usage_stats:
            self.usage_stats[model]["calls"] += 1
            self.usage_stats[model]["input_tokens"] += input_tokens
            self.usage_stats[model]["output_tokens"] += output_tokens
    
    def get_cost_estimate(self) -> dict:
        """Becs√ºlt k√∂lts√©g lek√©rdez√©se"""
        total_cost = 0.0
        breakdown = {}
        
        for model_name, stats in self.usage_stats.items():
            if model_name in MODELS:
                config = MODELS[model_name]
                input_cost = (stats["input_tokens"] / 1000) * config.cost_per_1k_input
                output_cost = (stats["output_tokens"] / 1000) * config.cost_per_1k_output
                model_cost = input_cost + output_cost
                total_cost += model_cost
                breakdown[model_name] = {
                    "calls": stats["calls"],
                    "input_tokens": stats["input_tokens"],
                    "output_tokens": stats["output_tokens"],
                    "cost_usd": round(model_cost, 4)
                }
        
        return {
            "total_cost_usd": round(total_cost, 4),
            "breakdown": breakdown
        }


# Singleton instance
_router: Optional[ModelRouter] = None

def get_model_router(default_model: str = "gpt-4o", force_model: str = None) -> ModelRouter:
    """Singleton router lek√©r√©se"""
    global _router
    if _router is None:
        _router = ModelRouter(default_model, force_model)
    return _router


# =====================================
#   CONVENIENCE FUNCTIONS
# =====================================

def route_model(user_message: str, context: str = "") -> str:
    """Egyszer≈± model routing wrapper"""
    router = get_model_router()
    return router.route(user_message, context)


def get_summary_model() -> str:
    """√ñsszefoglal√°shoz haszn√°land√≥ model"""
    return "gpt-4o-mini"


def get_reasoning_model() -> str:
    """Komplex gondolkod√°shoz haszn√°land√≥ model"""
    return "gpt-4o"


def get_embedding_model() -> str:
    """Embedding-hez haszn√°land√≥ model"""
    return "text-embedding-3-large"


# =====================================
#   DUAL-AGENT ARCHITECTURE
# =====================================
"""
üß† F≈êAGENT (GPT-4o): Gondolkod√°s, d√∂nt√©s, k√≥d √≠r√°s, elemz√©s
üß© H√ÅTT√âRAGENT (GPT-4o-mini): √ñsszefoglal√°s, mem√≥ria, kontextus t√∂m√∂r√≠t√©s

Ez biztos√≠tja:
- 128K token limit sosem lesz t√∫ll√©pve
- K√∂lts√©ghat√©kony m≈±k√∂d√©s
- Gyors h√°tt√©rm≈±veletek
"""

# Model nevek konstansok - DUAL AGENT ARCHITEKT√öRA
# THINKING: komplex feladatok (k√≥dol√°s, agentic, d√∂nt√©sek) - okosabb, lassabb
# WORKER: egyszer≈± feladatok (√∂sszefoglal√°s, mem√≥ria) - gyorsabb, olcs√≥bb
THINKING_MODEL = "gpt-4o"      # F≈ëagent - gondolkod√°s, agentic m√≥d
WORKER_MODEL = "gpt-4o-mini"   # H√°tt√©ragent - √∂sszefoglal√°s, mem√≥ria

# Kontextus limitek
MAX_MAIN_CONTEXT = 60000  # 60K token a f≈ëagentnek (van hely v√°laszra)
MAX_SUMMARY_CONTEXT = 20000  # 20K √∂sszefoglal√°sra
SUMMARY_TRIGGER_TOKENS = 40000  # Enn√©l t√∂bb token eset√©n t√∂m√∂r√≠t√ºnk


class DualAgentManager:
    """
    Dual-agent manager - koordin√°lja a f≈ë √©s h√°tt√©r agentet.
    
    üß† F≈êAGENT (GPT-4o):
    - Agentic tool calling
    - K√≥d gener√°l√°s
    - Komplex d√∂nt√©sek
    - Elemz√©s
    
    üß© H√ÅTT√âRAGENT (GPT-4o-mini):
    - Rolling summary gener√°l√°s
    - Kontextus t√∂m√∂r√≠t√©s
    - Mem√≥ria friss√≠t√©s
    - Fact extraction
    """
    
    def __init__(self, openai_client):
        self.client = openai_client
        self.thinking_model = THINKING_MODEL
        self.worker_model = WORKER_MODEL
        self.context_summary = ""
        self.accumulated_facts = []
        
    def compress_context_with_worker(self, messages: list, max_tokens: int = MAX_SUMMARY_CONTEXT) -> str:
        """
        üß© H√ÅTT√âRAGENT: Kontextus t√∂m√∂r√≠t√©se √∂sszefoglal√°ssal
        
        A GPT-4o-mini gyorsan √©s olcs√≥n k√©sz√≠t √∂sszefoglal√≥t a r√©gebbi √ºzenetekb≈ël.
        """
        if not messages:
            return ""
        
        # √úzenetek sz√∂vegg√© alak√≠t√°sa
        text_parts = []
        for msg in messages:
            role = msg.get("role", "unknown")
            content = msg.get("content", "")
            if content and role != "system":
                text_parts.append(f"[{role.upper()}]: {content[:2000]}")
        
        context_text = "\n\n".join(text_parts[-20:])  # Utols√≥ 20 √ºzenet
        
        if not context_text:
            return ""
        
        try:
            response = self.client.chat.completions.create(
                model=self.worker_model,
                messages=[
                    {
                        "role": "system",
                        "content": (
                            "Te egy prec√≠z √∂sszefoglal√≥ AI vagy. "
                            "K√©sz√≠ts T√ñM√ñR √∂sszefoglal√≥t a besz√©lget√©sr≈ël. "
                            "F√≥kusz√°lj: mit k√©rt a user, mit csin√°lt az asszisztens, mi t√∂rt√©nt a f√°jlokkal. "
                            "Max 500 sz√≥. Magyar nyelven."
                        )
                    },
                    {
                        "role": "user",
                        "content": f"Foglald √∂ssze ezt a besz√©lget√©st:\n\n{context_text}"
                    }
                ],
                max_tokens=800,
                temperature=0.3
            )
            
            summary = response.choices[0].message.content
            print(f"[WORKER AGENT] Context compressed: {len(context_text)} chars -> {len(summary)} chars")
            return summary
            
        except Exception as e:
            print(f"[WORKER AGENT] Compression error: {e}")
            return ""
    
    def extract_facts_with_worker(self, conversation: str) -> list:
        """
        üß© H√ÅTT√âRAGENT: Fontos t√©nyek kinyer√©se a besz√©lget√©sb≈ël
        """
        try:
            response = self.client.chat.completions.create(
                model=self.worker_model,
                messages=[
                    {
                        "role": "system",
                        "content": (
                            "Nyerd ki a FONTOS T√âNYEKET a besz√©lget√©sb≈ël. "
                            "Form√°tum: JSON lista [{\"fact\": \"...\", \"type\": \"file/decision/preference/bug\"}]. "
                            "Max 10 t√©ny. Csak a legfontosabbak!"
                        )
                    },
                    {
                        "role": "user",
                        "content": conversation[:5000]  # Max 5000 char
                    }
                ],
                max_tokens=500,
                temperature=0.2
            )
            
            import json
            content = response.choices[0].message.content
            # Try to parse JSON
            if "[" in content and "]" in content:
                json_str = content[content.find("["):content.rfind("]")+1]
                facts = json.loads(json_str)
                print(f"[WORKER AGENT] Extracted {len(facts)} facts")
                return facts
            return []
            
        except Exception as e:
            print(f"[WORKER AGENT] Fact extraction error: {e}")
            return []
    
    def should_compress(self, token_count: int) -> bool:
        """Kell-e t√∂m√∂r√≠teni a kontextust?"""
        return token_count > SUMMARY_TRIGGER_TOKENS
    
    def get_thinking_model(self) -> str:
        """üß† F≈ëagent model neve"""
        return self.thinking_model
    
    def get_worker_model(self) -> str:
        """üß© H√°tt√©ragent model neve"""
        return self.worker_model
    
    def build_optimized_context(
        self,
        system_prompt: str,
        history: list,
        user_message: str,
        token_manager=None
    ) -> list:
        """
        Optimaliz√°lt kontextus √©p√≠t√©s a dual-agent rendszerrel.
        
        Ha t√∫l nagy a kontextus, a h√°tt√©ragent t√∂m√∂r√≠ti.
        """
        messages = [{"role": "system", "content": system_prompt}]
        
        # Token sz√°mol√°s
        if token_manager:
            history_tokens = token_manager.count_messages_tokens(
                [{"role": m.get("role", "user"), "content": m.get("content", "")} for m in history]
            )
            
            if self.should_compress(history_tokens):
                print(f"[DUAL AGENT] History too large ({history_tokens} tokens), compressing...")
                
                # H√°tt√©ragent t√∂m√∂r√≠ti a r√©gi √ºzeneteket
                old_messages = history[:-6]  # R√©gi √ºzenetek
                recent_messages = history[-6:]  # Utols√≥ 6 megtart√°sa
                
                if old_messages:
                    summary = self.compress_context_with_worker(old_messages)
                    if summary:
                        messages.append({
                            "role": "system",
                            "content": f"[BESZ√âLGET√âS √ñSSZEFOGLAL√ì - kor√°bbi √ºzenetek t√∂m√∂r√≠tve]:\n{summary}"
                        })
                
                # Csak a friss √ºzenetek
                for msg in recent_messages:
                    messages.append({
                        "role": msg.get("role", "user"),
                        "content": msg.get("content", "")
                    })
            else:
                # Nincs t√∂m√∂r√≠t√©s, minden √ºzenet megy
                for msg in history:
                    messages.append({
                        "role": msg.get("role", "user"),
                        "content": msg.get("content", "")
                    })
        else:
            # Nincs token manager, egyszer≈± hozz√°ad√°s
            for msg in history:
                messages.append({
                    "role": msg.get("role", "user"),
                    "content": msg.get("content", "")
                })
        
        # User √ºzenet mindig megy
        messages.append({"role": "user", "content": user_message})
        
        return messages


# Singleton instance
_dual_agent_manager: DualAgentManager = None


def get_dual_agent_manager(openai_client=None) -> DualAgentManager:
    """Singleton dual agent manager"""
    global _dual_agent_manager
    if _dual_agent_manager is None and openai_client:
        _dual_agent_manager = DualAgentManager(openai_client)
    return _dual_agent_manager


def init_dual_agent(openai_client) -> DualAgentManager:
    """Dual agent inicializ√°l√°sa"""
    global _dual_agent_manager
    _dual_agent_manager = DualAgentManager(openai_client)
    print(f"[DUAL AGENT] Initialized: THINKING={THINKING_MODEL}, WORKER={WORKER_MODEL}")
    return _dual_agent_manager

