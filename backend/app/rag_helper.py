"""
RAG Helper - Smart Retrieval-Augmented Generation

Intelligens nagy f√°jl kezel√©s:
1. Automatikusan d√∂nt: teljes f√°jl vs RAG
2. Token-aware chunking
3. Relevancia alap√∫ context √©p√≠t√©s
"""

import os
import sys
from typing import List, Dict, Optional, Tuple

# Backend path hozz√°ad√°sa
BACKEND_DIR = os.path.dirname(os.path.dirname(__file__))
if BACKEND_DIR not in sys.path:
    sys.path.insert(0, BACKEND_DIR)

try:
    from vector_store import query_project, index_single_file, find_files_by_name, get_all_project_files
    HAS_VECTOR_STORE = True
except ImportError:
    HAS_VECTOR_STORE = False
    print("[RAG] Vector store not available")

try:
    from .token_manager import get_token_manager, TokenManager
    HAS_TOKEN_MANAGER = True
except ImportError:
    HAS_TOKEN_MANAGER = False


# =====================================
#   CONSTANTS
# =====================================

# Token thresholds
SMALL_FILE_TOKENS = 2000      # < 2k token: teljes f√°jl
MEDIUM_FILE_TOKENS = 10000    # 2k-10k token: √∂sszefoglal√°s + r√©szletek
LARGE_FILE_TOKENS = 30000     # 10k-30k token: csak RAG
HUGE_FILE_TOKENS = 50000      # > 50k token: figyelmeztet≈ë

# Context budget allocation
MAX_CONTEXT_TOKENS = 60000    # Max ~60k token a kontextusra (marad ~60k+ output-ra)
FILE_CONTEXT_RATIO = 0.6      # 60% a f√°jl kontextusra
RAG_CONTEXT_RATIO = 0.3       # 30% RAG r√©szletekre
HISTORY_RATIO = 0.1           # 10% chat history-ra


class RAGHelper:
    """
    Smart RAG kezel≈ë - automatikusan d√∂nt a f√°jl m√©rete alapj√°n
    """
    
    def __init__(self, project_name: str, project_root: str, model: str = "gpt-4o"):
        self.project_name = project_name
        self.project_root = project_root
        self.model = model
        
        if HAS_TOKEN_MANAGER:
            self.token_manager = get_token_manager(model)
        else:
            self.token_manager = None
    
    def count_tokens(self, text: str) -> int:
        """Token sz√°mol√°s"""
        if self.token_manager:
            return self.token_manager.count_tokens(text)
        # Fallback: ~4 karakter = 1 token
        return len(text) // 4
    
    def get_file_strategy(self, content: str) -> Dict:
        """
        Meghat√°rozza a f√°jl kezel√©si strat√©gi√°j√°t a m√©ret alapj√°n.
        
        Returns:
            {
                "strategy": "full" | "summary_plus_rag" | "rag_only" | "warning",
                "tokens": int,
                "recommendation": str
            }
        """
        tokens = self.count_tokens(content)
        
        if tokens < SMALL_FILE_TOKENS:
            return {
                "strategy": "full",
                "tokens": tokens,
                "recommendation": "Teljes f√°jl bet√∂lthet≈ë"
            }
        elif tokens < MEDIUM_FILE_TOKENS:
            return {
                "strategy": "summary_plus_rag",
                "tokens": tokens,
                "recommendation": "√ñsszefoglal√°s + RAG keres√©s aj√°nlott"
            }
        elif tokens < LARGE_FILE_TOKENS:
            return {
                "strategy": "rag_only",
                "tokens": tokens,
                "recommendation": "Csak RAG keres√©s - t√∫l nagy a teljes bet√∂lt√©shez"
            }
        else:
            return {
                "strategy": "warning",
                "tokens": tokens,
                "recommendation": f"‚ö†Ô∏è NAGYON NAGY F√ÅJL ({tokens} token) - chunked processing sz√ºks√©ges"
            }
    
    def build_smart_context(
        self,
        query: str,
        active_file_path: Optional[str] = None,
        active_file_content: Optional[str] = None,
        max_tokens: int = MAX_CONTEXT_TOKENS
    ) -> Dict:
        """
        Intelligens kontextus √©p√≠t√©s.
        
        Returns:
            {
                "context": str,          # A v√©gs≈ë kontextus sz√∂veg
                "tokens_used": int,
                "strategy_used": str,
                "files_included": List[str],
                "rag_chunks": int
            }
        """
        context_parts = []
        files_included = []
        rag_chunks_count = 0
        tokens_used = 0
        strategy_used = "none"
        
        # Budget kisz√°m√≠t√°sa
        file_budget = int(max_tokens * FILE_CONTEXT_RATIO)
        rag_budget = int(max_tokens * RAG_CONTEXT_RATIO)
        
        # 1. Akt√≠v f√°jl kezel√©se
        if active_file_content:
            strategy = self.get_file_strategy(active_file_content)
            strategy_used = strategy["strategy"]
            
            if strategy["strategy"] == "full":
                # Teljes f√°jl
                context_parts.append(f"=== AKT√çV F√ÅJL: {active_file_path} ===\n{active_file_content}\n")
                tokens_used += strategy["tokens"]
                files_included.append(active_file_path)
                
            elif strategy["strategy"] == "summary_plus_rag":
                # Eleje + v√©ge + RAG
                lines = active_file_content.split('\n')
                summary = self._create_file_summary(lines, active_file_path, file_budget // 2)
                context_parts.append(summary)
                tokens_used += self.count_tokens(summary)
                files_included.append(active_file_path)
                
                # RAG search a r√©szletek√©rt
                if HAS_VECTOR_STORE:
                    rag_results = self._rag_search(query, rag_budget // 2)
                    if rag_results:
                        context_parts.append("\n=== RELEV√ÅNS K√ìDR√âSZLETEK (RAG) ===\n")
                        context_parts.append(rag_results["context"])
                        tokens_used += rag_results["tokens"]
                        rag_chunks_count += rag_results["chunk_count"]
                        
            elif strategy["strategy"] in ("rag_only", "warning"):
                # Csak strukt√∫ra + RAG
                lines = active_file_content.split('\n')
                structure = self._extract_structure(lines, active_file_path)
                context_parts.append(structure)
                tokens_used += self.count_tokens(structure)
                files_included.append(f"{active_file_path} (strukt√∫ra)")
                
                # RAG search
                if HAS_VECTOR_STORE:
                    rag_results = self._rag_search(query, rag_budget)
                    if rag_results:
                        context_parts.append("\n=== RELEV√ÅNS K√ìDR√âSZLETEK (RAG) ===\n")
                        context_parts.append(rag_results["context"])
                        tokens_used += rag_results["tokens"]
                        rag_chunks_count += rag_results["chunk_count"]
        
        else:
            # Nincs akt√≠v f√°jl - csak RAG
            if HAS_VECTOR_STORE:
                rag_results = self._rag_search(query, rag_budget)
                if rag_results:
                    context_parts.append("=== RELEV√ÅNS K√ìDR√âSZLETEK (RAG) ===\n")
                    context_parts.append(rag_results["context"])
                    tokens_used += rag_results["tokens"]
                    rag_chunks_count += rag_results["chunk_count"]
                    strategy_used = "rag_only"
        
        return {
            "context": "\n".join(context_parts),
            "tokens_used": tokens_used,
            "strategy_used": strategy_used,
            "files_included": files_included,
            "rag_chunks": rag_chunks_count
        }
    
    def _create_file_summary(self, lines: List[str], file_path: str, max_tokens: int) -> str:
        """F√°jl √∂sszefoglal√°s: eleje + v√©ge + statisztika"""
        total_lines = len(lines)
        
        # Sz√°m√≠tsuk ki h√°ny sort f√©r√ºnk bele
        tokens_per_line = 10  # Becsl√©s
        available_lines = max_tokens // tokens_per_line
        
        head_lines = min(available_lines // 2, 100)
        tail_lines = min(available_lines // 2, 50)
        
        summary_parts = [
            f"=== F√ÅJL √ñSSZEFOGLAL√ÅS: {file_path} ===",
            f"√ñsszesen: {total_lines} sor",
            "",
            f"--- ELEJE ({head_lines} sor) ---",
            "\n".join(lines[:head_lines]),
            "",
            f"... [{total_lines - head_lines - tail_lines} sor kihagyva] ...",
            "",
            f"--- V√âGE ({tail_lines} sor) ---",
            "\n".join(lines[-tail_lines:]),
        ]
        
        return "\n".join(summary_parts)
    
    def _extract_structure(self, lines: List[str], file_path: str) -> str:
        """K√≥d strukt√∫ra kinyer√©se (f√ºggv√©nyek, oszt√°lyok)"""
        structure_lines = [
            f"=== F√ÅJL STRUKT√öRA: {file_path} ({len(lines)} sor) ===",
            ""
        ]
        
        # Python/JS/TS strukt√∫ra elemz√©s
        for i, line in enumerate(lines, 1):
            stripped = line.strip()
            
            # Python
            if stripped.startswith(('def ', 'class ', 'async def ')):
                structure_lines.append(f"L{i}: {stripped}")
            # JavaScript/TypeScript
            elif stripped.startswith(('function ', 'const ', 'export ', 'class ')):
                if 'function' in stripped or '=>' in stripped or 'class ' in stripped:
                    structure_lines.append(f"L{i}: {stripped[:100]}")
            # Comments that look like section headers
            elif stripped.startswith(('# ===', '// ===', '/* ===', '# ---', '// ---')):
                structure_lines.append(f"L{i}: {stripped}")
        
        if len(structure_lines) <= 2:
            structure_lines.append("(Nem tal√°ltam explicit strukt√∫ra elemeket)")
        
        return "\n".join(structure_lines)
    
    def _rag_search(self, query: str, max_tokens: int) -> Optional[Dict]:
        """RAG keres√©s v√©grehajt√°sa"""
        if not HAS_VECTOR_STORE:
            return None
        
        try:
            # Semantic search
            results = query_project(self.project_name, query, top_k=10)
            
            if not results:
                return None
            
            context_parts = []
            tokens_used = 0
            chunks_included = 0
            
            for r in results:
                chunk_text = f"[{r['file_path']}:{r['chunk_index']}] (relevancia: {r['score']:.2f})\n{r['content']}\n---\n"
                chunk_tokens = self.count_tokens(chunk_text)
                
                if tokens_used + chunk_tokens > max_tokens:
                    break
                
                context_parts.append(chunk_text)
                tokens_used += chunk_tokens
                chunks_included += 1
            
            if not context_parts:
                return None
            
            return {
                "context": "\n".join(context_parts),
                "tokens": tokens_used,
                "chunk_count": chunks_included
            }
            
        except Exception as e:
            print(f"[RAG] Search error: {e}")
            return None
    
    def search_relevant_code(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        Relev√°ns k√≥d keres√©se a projektben.
        Haszn√°lhat√≥ az agentic tools-b√≥l.
        """
        if not HAS_VECTOR_STORE:
            return []
        
        try:
            return query_project(self.project_name, query, top_k=top_k)
        except Exception as e:
            print(f"[RAG] Search error: {e}")
            return []
    
    def index_file(self, rel_path: str) -> Dict:
        """Egyetlen f√°jl indexel√©se (ment√©s ut√°n)"""
        if not HAS_VECTOR_STORE:
            return {"status": "skipped", "reason": "no_vector_store"}
        
        try:
            return index_single_file(self.project_name, self.project_root, rel_path)
        except Exception as e:
            return {"status": "error", "error": str(e)}


def get_rag_helper(project_name: str, project_root: str, model: str = "gpt-4o") -> RAGHelper:
    """Factory function for RAGHelper"""
    return RAGHelper(project_name, project_root, model)


# =====================================
#   CONVENIENCE FUNCTIONS
# =====================================

def should_use_rag(content: str, model: str = "gpt-4o") -> bool:
    """Egyszer≈± d√∂nt√©s: kell-e RAG a f√°jlhoz?"""
    if HAS_TOKEN_MANAGER:
        tm = get_token_manager(model)
        tokens = tm.count_tokens(content)
    else:
        tokens = len(content) // 4
    
    return tokens > SMALL_FILE_TOKENS


def get_file_handling_recommendation(content: str, model: str = "gpt-4o") -> str:
    """Aj√°nl√°s a f√°jl kezel√©s√©re"""
    if HAS_TOKEN_MANAGER:
        tm = get_token_manager(model)
        tokens = tm.count_tokens(content)
    else:
        tokens = len(content) // 4
    
    if tokens < SMALL_FILE_TOKENS:
        return f"‚úÖ Kis f√°jl ({tokens} token) - teljes bet√∂lt√©s OK"
    elif tokens < MEDIUM_FILE_TOKENS:
        return f"‚ö†Ô∏è K√∂zepes f√°jl ({tokens} token) - √∂sszefoglal√°s + RAG aj√°nlott"
    elif tokens < LARGE_FILE_TOKENS:
        return f"üî∂ Nagy f√°jl ({tokens} token) - csak RAG keres√©s"
    else:
        return f"üî¥ NAGYON NAGY ({tokens} token) - chunked processing sz√ºks√©ges!"

